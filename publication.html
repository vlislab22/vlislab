<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Visual Learning and Intelligent Systems (VLIS) LAB</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Nav -->
			<nav id="nav">
				<ul class="container">
					<li><a href="./index.html">Home</a></li>
					<li><a href="./research.html">Research</a></li>
					<li><a href="./people.html">People</a></li>
					<li><a href="./publication.html">Publication</a></li>
					<li><a href="./project.html">Project</a></li>
					<li><a href="./news.html">News</a></li>
					<li><a href="./opensource.html">Open Source</a></li>
					<li><a href="./contact.html">Contact</a></li>
				</ul>
			</nav>



	<!-- Intro -->
		<section id="intro" class="wrapper style3 fullscreen fade-up" >
			<div class="inner">
				<h1><strong>Publications</strong></h1>
				<p><strong>We aim to publish the most top-tier research in world-class conferences and journals.</strong></p>
				<ul class="actions">
				<li><a href="#one" class="button scrolly">Learn more</a></li>
				</ul>
			</div>
		</section>

		<!-- Highlights -->
			<section>
				<ul class="divided">
					<li>


						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout</a></h3>
						</header>
						<a class="image left"><img src="images/CompoNeRF.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv</strong></p>
						<p align="left"><strong>Keywords: NeRF, text-to-image generation, diffusuion, 3D content </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2303.13843.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>


						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout</a></h3>
						</header>
						<a class="image left"><img src="images/pami-36o.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv</strong></p>
						<p align="left"><strong>Keywords: NeRF, text-to-image generation, diffusuion, 3D content </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2303.13843.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks</a></h3>
						</header>
						<a class="image left"><img src="images/pami-36o.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Xu Zheng, Yexin Liu, Yunfan Lu, Tongyan Hua, Tianbo Pan, Weiming Zhang, Dacheng Tao, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv</strong></p>
						<p align="left"><strong>Keywords: event-based vision, deep learning, benchmarks </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2302.08890.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution</a></h3>
						</header>
						<a class="image left"><img src="images/pami-36o.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yunfan Lu, Zipeng Wang, Minjie Liu, Hongjian Wang, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2023</strong></p>
						<p align="left"><strong>Keywords: event-based vision, video super-resolution </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2303.13767.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">HRDFuse: Monocular 360Â°Depth Estimation by Collaboratively Learning Holistic-with-Regional Depth Distributions</a></h3>
						</header>
						<a class="image left"><img src="images/pami-36o.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Hao Ai, Zidong cao, Yan-pei Cao, Ying Shan, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2023</strong></p>
						<p align="left"><strong>Keywords: 360 vision, depth estimation </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2303.11616.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective</a></h3>
						</header>
						<a class="image left"><img src="images/pami-36o.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Jinjing Zhu, Haotian Bai, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2023 (Highlight top 2.5%)</strong></p>
						<p align="left"><strong>Keywords: UDA, game theory </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2303.13434.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>




						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation</a></h3>
						</header>
						<a class="image left"><img src="images/pami-36o.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Xu Zheng, Jinjing Zhu, Yexin Liu, Zidong Cao, Chong Fu, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2023</strong></p>
						<p align="left"><strong>Keywords: 360 vision, UDA, Segmentation </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2303.14360.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Vetaverse: A Survey on the Intersection of Metaverse, Vehicles, and Transportation Systems</a></h3>
						</header>
						<a class="image left"><img src="images/pami-36o.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Pengyuan Zhou, Jinjing Zhu, Yiting Wang, Yunfan Lu, Zixiang Wei, Haolin Shi, Yuchen Ding, Yu Gao, Qinglong Huang, Yan Shi, Ahmad Alhilal, Lik-Hang Lee, Tristan Braud, Pan Hui, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>AAAI 2023</strong></p>
						<p align="left"><strong>Keywords: Metaverse, Transportation </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2210.15109.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>





						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">SEPT: Towards Scalable and Efficient Visual Pre-Training</a></h3>
						</header>
						<a class="image left"><img src="images/pami-36o.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yiqi Lin, Huabin Zheng, Huaping Zhong, Jinjing Zhu, Weijia Li, Conghui He, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>AAAI 2023</strong></p>
						<p align="left"><strong>Keywords: CNN, model pretraining, image classification  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2212.05473.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Transformer-CNN Cohort: Semi-supervised Semantic Segmentation by the Best of
Both Students</a></h3>
						</header>
						<a class="image left"><img src="images/pami-36o.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Xu Zheng, Yunhao Luo, Hao Wang, Chong Fu and Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv</strong></p>
						<p align="left"><strong>Keywords: CNN, Transformer, Semantic segmentation  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2209.02178.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Deep Learning for Omnidirectional Vision: A Survey and New Perspectives</a></h3>
						</header>
						<a class="image left"><img src="images/pami-36o.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Hao Ai*, Zidong CaoË, Jinjing Zhu, Haotian Bai, Yucheng Chen, and Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv (TPAMI in review)</strong></p>
						<p align="left"><strong>Keywords: 360 vision, deep learning, survey and analysis  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2205.10468.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>


						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">All One Needs to Know about Priors for Deep Image Restoration and Enhancement: A Survey</a></h3>
						</header>
						<a class="image left"><img src="images/pami-prior.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Changgyoon Oh, Wonjune Cho, Daehee Park, Yujeong Chae, Lin Wang, Kuk-Jin Yoon</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv (TPAMI in review)</strong></p>
						<p align="left"><strong>Keywords: Prior, Image restoration and  enhancement, survey and alalysis </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2206.02070.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Efficient Video Deblurring Guided by Motion Magnitude</a></h3>
						</header>
						<a class="image left"><img src="images/eccv_mmp.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yusheng Wang, Yunfan Lu, Ye Gao, Lin Wang, Zhihang Zhong, Yinqiang Zheng, and Atsushi Yamashita</strong></p>
						<!-- -->
						<p align="left"><strong>ECCV 2022</strong></p>
						<p align="left"><strong>Keywords: Video deblurring, motion magnitude</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2112.06179.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">BIPS: Bi-modal Indoor Panorama Synthesis via Residual Depth-aided Adversarial Learning</a></h3>
						</header>
						<a class="image left"><img src="images/bips.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Changgyoon Oh, Wonjune Cho, Daehee Park, Yujeong Chae, Lin Wang, Kuk-Jin Yoon</strong></p>
						<!-- -->
						<p align="left"><strong>ECCV 2022</strong></p>
						<p align="left"><strong>Keywords: Panorama synthesis,  bi-modal fusion, adversarial learning  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2112.06179.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>




						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Event-guided Deblurring of Unknown Exposure Time Videos</a></h3>
						</header>
						<a class="image left"><img src="images/deblur_event.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Taewoo Kim, Jungmin Lee, Lin Wang and Kuk-Jin Yoon</strong></p>
						<!-- -->
						<p align="left"><strong>ECCV 2022</strong></p>
						<p align="left"><strong>Keywords: Event-based vision, video deblurring, video frame acquisition  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2112.06988.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">SphereSR: 360Â° Image Super-Resolution with Arbitrary Projection via Continuous Spherical Image Representation</a></h3>
						</header>
						<a class="image left"><img src="images/sphereSR.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Youngho Yoon, Lin Wang, Inchul Chung and Kuk-Jin Yoon</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2022</strong></p>
						<p align="left"><strong>Keywords: Ominidirectional vision,  Image super-resolution, Continuous image representation  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2112.06179.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>




						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Deep Learning for HDR Imaging: State-of-the-Art and Future Trends</a></h3>
						</header>
						<a class="image left"><img src="images/hdr_img.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
						<!-- -->
						<p align="left"><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI, IF: 17.861)</strong></p>
						<p align="left"><strong>Keywords: High dynamic range imaging, Convolutional Neural Networks (CNNs), Deep Learning  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2110.10394.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
							<article class="box highlight">
								<header>
									<h3><a href="#">All One Needs to Know about Metaverse: A Complete Survey on Technological Singularity, Virtual Ecosystem, and Research Agenda</a></h3>
								</header>
								<a class="image left"><img src="images/metaverse.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
						    <div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
								<p align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></p>
							<!-- -->
								<p align="left"><strong>The proceedings of IEEE (IF: 10.961)</strong></p>
								<p align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></p>
								<ul class="actions" align="left">
								<li><a href="https://tuhat.helsinki.fi/ws/portalfiles/portal/169348619/METAVERSE.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
								</ul>
								</article>

								<!-- Highlight -->
								<article class="box highlight">
								<header>
								<h3><a href="#">SiamEvent:Event-based Object Tracking via Edge-aware Similarity Learning with Siamese Networks</a></h3>
								</header>
								<a class="image left"><img src="images/siamevent.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
									<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
									<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
									<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
								<p align="left"><strong>Yujeon Chae, Lin Wang and  Kuk-Jin Yoon</strong></p>
								<!-- -->
								<p align="left"><strong>IEEE RA-L and ICRA, 2022.</strong></p>
								<p align="left"><strong>Keywords: Event-based vision, object tracking</strong></p>
								<ul class="actions" align="left">
								<li><a href="https://arxiv.org/pdf/2109.13456.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
								</ul>
								</article>

							<!-- Highlight -->
							<article class="box highlight">
								<header>
									<h3><a href="#">Semi-supervised Transfer Learning for Single Image Super-Resolution via Feature Augmentation and Inverse Consistency Regularization</a></h3>
								</header>
								<a class="image left"><img src="images/dbep_results.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
						    <div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
								<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
							<!-- -->
								<p align="left"><strong>the Association for the Advancement of Artificial Intelligence (AAAI), 2022.</strong></p>
								<p align="left"><strong>Keywords: Image super-resolution, Semi-supervised learning, transfer learning</strong></p>
								<ul class="actions" align="left">
								<li><a href="#" class="button icon solid fa-file-pdf"> Check our paper</a></li>
								</ul>
								</article>


								<!-- Highlight -->
								<article class="box highlight">
								<header>
								<h3><a href="#">Joint Framework for Intensity Image Reconstruction and Super-Resolution with an Event Camera</a></h3>
								</header>
								<a class="image left"><img src="images/eventsr_PAMI.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
									<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
									<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
									<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
								<p align="left"><strong>Lin Wang, Tae-Kyun Kim and Kuk-Jin Yoon</strong></p>
								<!-- -->
								<p align="left"><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI, IF: 17.861)</strong></p>
								<p align="left"><strong>Keywords: Adversarial attack, Holistic scene understanding, Multi-task learning, Generative model </strong></p>
								<ul class="actions" align="left">
								<li><a href="https://ieeexplore.ieee.org/abstract/document/9524508" class="button icon solid fa-file-pdf"> Check our paper</a></li>
								</ul>
								</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">PSAT-GAN: Efficient Adversarial Attacks against Holistic Scene Understanding</a></h3>
							</header>
							<a class="image left"><img src="images/psatgan.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong> IEEE Transactions on Image Processing (TIP,  IF 10.856)</strong></p>
							<p align="left"><strong>Keywords: Adversarial attack, Holistic scene understanding, Multi-task learning, Generative model </strong></p>
							<ul class="actions" align="left">
							<li><a href="https://ieeexplore.ieee.org/document/9524508" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Semi-supervised Student-Teacher Learning for Single Image Super-Resolution</a></h3>
							</header>
							<a class="image left"><img src="images/semi_sup_PR.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>Pattern Recognition (PR, IF 7.74)</strong></p>
							<p align="left"><strong>Keywords: Image super-resolution, Semi-supervised learning, Student-Teacher learning, Adversarial Learning.</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320321003873" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Dual Transfer Learning for Event-based End-task Prediction via Pluggable Image Translation</a></h3>
							</header>
							<a class="image left"><img src="images/dtl.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang, YuJeon Chae and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>International Conference on Computer Vision (ICCV), 2021.</strong></p>
							<p align="left"><strong>Keywords: Event-based vision, dual-transfer learning, event to image translation</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Dual_Transfer_Learning_for_Event-Based_End-Task_Prediction_via_Pluggable_Event_ICCV_2021_paper.pdf" class="button icon solid fa-file"> Check our paper</a></li>
							</ul>
							</article>

							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">GraphShop: Graph-based Approach for Shop-type Recommendation</a></h3>
							</header>
							<a class="image left"><img src="images/graphshop.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Guoyuan An, Sungeui Yoon,  Jaeyoon Kim, Lin Wang and Myoungho Kim </strong></p>
							<!-- -->
							<p align="left"><strong>SIAM International Conference on Data Mining</strong></p>
							<p align="left"><strong>Keywords: shop-type recommendation, graph neural network, smart city, recommender system </strong></p>
							<ul class="actions" align="left">
							<li><a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611976700.7" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">EvDistill: Asynchronous Events to End-task Learning via Bidirectional Reconstruction-guided Cross-modal Knowledge Distillation</a></h3>
							</header>
							<a class="image left"><img src="images/evdistill.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang, YuJeon Chae, Sunghoon Yoon, Tae-Kyun Kim and  Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>The Conference on Computer Vision and Pattern Recognition (CVPR)  2021</strong></p>
							<p align="left"><strong>Keywords: Event-based vision, Knowledge distillation, Semantic segmentation, Generative model</strong></p>
							<ul class="actions" align="left">
							<li><a href="http://vi.kaist.ac.kr/wp-content/uploads/2021/03/EventSeg_S_Tlearning.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks </a></h3>
							</header>
							<a class="image left"><img src="images/kd_pami.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI, IF 17.861)</strong></p>
							<p align="left"><strong>Keywords: knowledge distillation, student-teacher learning, visual intelligence</strong></p>
							<ul class="actions" align="left">
							<li><a href="http://vi.kaist.ac.kr/wp-content/uploads/2021/03/EventSeg_S_Tlearning.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Learning to Reconstruct HDR Images from Events, with Applications to Depth and Flow</a></h3>
							</header>
							<a class="image left"><img src="images/eventgan_ijcv.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>S. Mohammad Mostafavi I.,  Lin Wang, and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>International Journal of Computer  Vision (IJCV, IF: 7.410)</strong></p>
							<p align="left"><strong>Keywords: Event-based vision, HDR image reconstruction, depth and flow estimation</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://link.springer.com/article/10.1007/s11263-020-01410-2" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">EventSR: from asynchronous events to image reconstruction, restoration, and super-resolution via end-to-end adversarial learning</a></h3>
							</header>
							<a class="image left"><img src="images/eventsr.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang, Tae-Kyun Kim and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>The Conference on Computer Vision and Pattern Recognition (CVPR) 2020</strong></p>
							<p align="left"><strong>Keywords: Event-based vision, image reconstruction, restoration, super-resolution, GAN</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_EventSR_From_Asynchronous_Events_to_Image_Reconstruction_Restoration_and_Super-Resolution_CVPR_2020_paper.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Deceiving Image-to-Image Translation Networks for Autonomous Driving With Adversarial Perturbations</a></h3>
							</header>
							<a class="image left"><img src="images/adv_icra.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
							<p align="left"><strong>Lin Wang, Wonjune Cho and Kuk-Jin Yoon</strong></p>
							<p align="left"><strong>International Conference on Robotics and Automation (ICRA 2020) and IEEE Robotics and Automation Letters (RA-L,  IF: 3.608)</strong></p>
							<p align="left"><strong>Keyworks: adversarial attack,  Image to image translation, Autonomous driving</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://ieeexplore.ieee.org/document/8962221" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>

							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Event-based High Dynamic Range Image and Very High Frame Rate Video Generation using Conditional Generative Adversarial Networks</a></h3>
							</header>
							<a class="image left"><img src="images/eventgan_cvpr.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
							<p align="left"><strong>Lin Wang, S. Mohammad Mostafavi I., Yo-Sung Ho, and Kuk-Jin Yoon</strong></p>
							<p align="left"><strong>The Conference on Computer Vision and Pattern Recognition (CVPR)  2019</strong></p>
							<p align="left"><strong>Keywords: event-based vision, image reconstruction, high-framerate video, GAN</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Event-Based_High_Dynamic_Range_Image_and_Very_High_Frame_Rate_CVPR_2019_paper.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>

							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Co-DesignMR: A MR-based Interactive Workstation Design System Supporting Collaboration</a></h3>
							</header>
							<a class="image left"><img src="images/coaugMR.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
							<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
							<p align="left"><strong>Arxiv 2020</strong></p>
							<p align="left"><strong>Keywords: Mixed reality, ergonomics, product design</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://arxiv.org/pdf/1907.03107.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>

							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Visual simulation of a capsizing ship in stormy weather condition</a></h3>
							</header>
							<a class="image left"><img src="images/vis_comp.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
							<p align="left"><strong>Lin Wang and Soonhung Han</strong></p>
							<p align="left"><strong>The Visual Computer </strong></p>
							<p align="left"><strong>Keywords: Ship capsize, fluid simulation, visual effects</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://link.springer.com/content/pdf/10.1007/s00371-018-1579-6.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">A visual simulation of ocean floating wind power system</a></h3>
							</header>
							<a class="image left"><img src="images/float.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
							<p align="left"><strong>Lin Wang, Hyuncheol Kim, Imgyu Kim and Soonhung Han</strong></p>
							<p align="left"><strong>Computer Animation and Virtual Worlds</strong></p>
							<p align="left"><strong>Keywords: Wind turbine design, visual simulation, physically-based fluid simulation</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/cav.1859" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>







					</li>


				</ul>
			</section>





		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
