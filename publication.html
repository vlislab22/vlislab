<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Visual Learning and Intelligent Systems (VLIS) LAB</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Nav -->
			<nav id="nav">
				<ul class="container">
					<li><a href="./index.html">Home</a></li>
					<li><a href="./research.html">Research</a></li>
					<li><a href="./people.html">People</a></li>
					<li><a href="./publication.html">Publication</a></li>
					<li><a href="./project.html">Teaching</a></li>
					<li><a href="./news.html">News</a></li>
					<li><a href="./opensource.html">Open Source</a></li>
					<li><a href="./contact.html">Contact</a></li>
				</ul>
			</nav>



	<!-- Intro -->
		<section id="intro" class="wrapper style3 fullscreen fade-up" >
			<div class="inner">
				<h1><strong>Publications</strong></h1>
				<p><strong>We aim to publish the most top-tier research in world-class conferences and journals.</strong></p>
				<ul class="actions">
				<li><a href="#one" class="button scrolly">Learn more</a></li>
				</ul>
			</div>
		</section>

		<!-- Highlights -->
			<section>
				<ul class="divided">
					<li>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Energy-based Domain-Adaptive Segmentation with Depth Guidance</a></h3>
						</header>
						<a class="image left"><img src="images/SMART.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong> Jinjing Zhu, Zhedong Hu, Tae-Kyun Kim, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>IEEE RA-L 2024 </strong></p>
						<p align="left"><strong>Keywords: Unsupervised Domain Adaptation, Semantic Segmentation, Energy-based Model </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2402.03795" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">360∘ High-Resolution Depth Estimation via Uncertainty-aware Structural Knowledge Transfer</a></h3>
						</header>
						<a class="image left"><img src="images/Uncertainty_360.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Zidong Cao, Hao Ai, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv</strong></p>
						<p align="left"><strong>Keywords: omnidirectional image, depth estimation, image super-resolution, knowledge transfer, weakly-supervised learning</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2304.07967" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Disentangled Cross-modal Fusion for Event-guided Image Super-Resolution</a></h3>
						</header>
						<a class="image left"><img src="images/EGI-SR.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong> Minjie Liu, Hongjian Wang, Kuk-Jin Yoon, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>IEEE TAI 2024 </strong></p>
						<p align="left"><strong>Keywords: Image Super-Resolution, Event-Based Vision </strong></p>
						<ul class="actions" align="left">
						<li><a href="" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">ClassLIE: Structure- and Illumination-Adaptive Classification for Low-Light Image Enhancement</a></h3>
						</header>
						<a class="image left"><img src="images/ClassLIE.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong> Zixiang Wei, Yiting Wang, Lichao Sun, Athanasios V. Vasilakos, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>IEEE TAI 2024 </strong></p>
						<p align="left"><strong>Keywords: Low-Light Image Enhancement </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2312.13265" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Co-Occ: Coupling Explicit Feature Fusion with Volume Rendering Regularization for Multi-Modal 3D Semantic Occupancy Prediction</a></h3>
						</header>
						<a class="image left"><img src="images/Co-Occ.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong> Jingyi Pan, Zipeng Wang, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>IEEE RA-L 2024 </strong></p>
						<p align="left"><strong>Keywords: 3D Semantic Occupancy Prediction </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2404.04561.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">MYCloth: Towards Intelligent and Interactive Online T-Shirt Customization based on Uer's Preference</a></h3>
						</header>
						<a class="image left"><img src="images/MYCloth.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong> Yexin Liu, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>IEEE CAI 2024 </strong></p>
						<p align="left"><strong>Keywords: Online Customization, Interactive </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2404.15801" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Interact360: Interactive Human-centered Text to 360 Panorama Generation</a></h3>
						</header>
						<a class="image left"><img src="images/Interact360.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong> Zeyu Cai, Zhelong Huang, Xu Zheng, Yexin Liu, Chao Liu, Zeyu Wang, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>IEEE CAI 2024 </strong></p>
						<p align="left"><strong>Keywords: Text-to-360 </strong></p>
						<ul class="actions" align="left">
						<li><a href="" class="button icon solid fa-file-pdf"> Coming Soon!</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D Synthesis</a></h3>
						</header>
						<a class="image left"><img src="images/BrightDreamer.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong> Lutao Jiang, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv </strong></p>
						<p align="left"><strong>Keywords: 3D Gaussian, Text-to-3D </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2403.11273" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation</a></h3>
						</header>
						<a class="image left"><img src="images/Imganything.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong> Yuanhuiyi Lyu, Xu Zheng, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv </strong></p>
						<p align="left"><strong>Keywords: Multi-modal Image Generation</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2401.17664" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition</a></h3>
						</header>
						<a class="image left"><img src="images/EventDance.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong> Xu Zheng, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2024 </strong></p>
						<p align="left"><strong>Keywords: UDA, Object Recognition</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2403.14082" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model for Distortion-aware Panoramic Semantic Segmentation</a></h3>
						</header>
						<a class="image left"><img src="images/GoodSAM.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong> Weiming Zhang, Yexin Liu, Xu Zheng, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2024 </strong></p>
						<p align="left"><strong>Keywords: Panoramic Semantic Segmentation, SAM</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2403.16370" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More</a></h3>
						</header>
						<a class="image left"><img src="images/ExACT.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Jiazhou Zhou, Xu Zheng, Yuanhuiyi Lyu, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2024 </strong></p>
						<p align="left"><strong>Keywords: Language-guided, Action Recognition</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2403.12534" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">How the Hell does NeRF Matter for Real-Time RGB-D SLAM: A Novel Benchmark for Scene Representation and Geometric Rendering</a></h3>
						</header>
						<a class="image left"><img src="images/SLAM_Benchmark.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Tongyan Hua, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2024 </strong></p>
						<p align="left"><strong>Keywords: Benchmark, RGB-D SLAM</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2403.19473" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All</a></h3>
						</header>
						<a class="image left"><img src="images/UniBind.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yuanhuiyi Lyu, Xu Zheng, Jiazhou Zhou, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2024 </strong></p>
						<p align="left"><strong>Keywords: Multi-modal Learning, LLM</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2403.12532" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Towars Robust Event-guided Low-Light Image Enhancement: A Large-Scale Real-World Event-Image Dataset and Novel Approach</a></h3>
						</header>
						<a class="image left"><img src="images/Low_light_benchmark.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Guoqiang Liang, Kanghao Chen, Hangyu Li, Yunfan Lu, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2024 </strong></p>
						<p align="left"><strong>Keywords: Low-Light Enhancement, Event</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2404.00834" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and Distance-Aware Bi-Projection Fusion</a></h3>
						</header>
						<a class="image left"><img src="images/Elite360D.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Hao Ai, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2024 </strong></p>
						<p align="left"><strong>Keywords: 360 Depth Estimation</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2403.16376" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation</a></h3>
						</header>
						<a class="image left"><img src="images/Style_matter.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Xu Zheng, Pengyuan Zhou, Athanasios Vasilakos, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2024 </strong></p>
						<p align="left"><strong>Keywords: UDA/ Panoramic Segmentation</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2403.12505" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>


						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Chasing Day and Night: Towards Robust and Efficient All-Day Object Detection Guided by an Event Camera</a></h3>
						</header>
						<a class="image left"><img src="images/CDN.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Jiahang Cao, Xu Zheng, Yuanhuiyi Lyu, Jiaxu Wang, Renjing Xu, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>ICRA 2024 </strong></p>
						<p align="left"><strong>Keywords: Object Detection, Event</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2309.09297" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">SRFNet: Monocular Depth Estimation with Fine-grained Structure via Spatial Reliability-oriented Fusion of Frames and Events</a></h3>
						</header>
						<a class="image left"><img src="images/SRFNet.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Tianbo Pan, Zidong Cao, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>ICRA 2024 </strong></p>
						<p align="left"><strong>Keywords: Depth Estimation, Event</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2309.12842" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Transformer-CNN Cohort: Semi-supervised Semantic Segmentation by the Best of Both Students</a></h3>
						</header>
						<a class="image left"><img src="images/Transformer.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Xu Zheng, Yunhao Luo, Chong Fu, Kangcheng Liu, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>ICRA 2024</strong></p>
						<p align="left"><strong>Keywords: CNN, Transformer, Semantic segmentation  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2209.02178.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity Monocular Dense Mapping</a></h3>
						</header>
						<a class="image left"><img src="images/FMapping.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Tongyan Hua, Haotian Bai, Zidong Cao, Ming Liu, Dacheng Tao, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv</strong></p>
						<p align="left"><strong>Keywords: SLAM, NeRF </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2401.03203.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Dream360: Diverse and Immersive Outdoor Virtual Scene Creation via Transformer-Based 360$^\circ$ Image Outpainting</a></h3>
						</header>
						<a class="image left"><img src="images/Dream360.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Hao Ai, Zidong Cao, Haonan Lu, Chen Chen, Jian Ma, Pengyuan Zhou, Tae-Kyun Kim, Pan Hui, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>IEEE VR 2024</strong></p>
						<p align="left"><strong>Keywords: Panorama images,Virtual Scene Creation.</strong></p>
						<ul class="actions" align="left">
						<li><a href="" class="button icon solid fa-file-pdf"> Coming Soon!</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Swift-Eye: Towards Anti-blink Pupil Tracking for Precise and Robust High-Frequency Near-Eye Movement Analysis with Event Cameras</a></h3>
						</header>
						<a class="image left"><img src="images/Swift_Eye.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Tongyu Zhang, Yiran Shen, Guangrong Zhao, Lin Wang, Xiaoming Chen, Lu Bai, Yuanfeng Zhou</strong></p>
						<!-- -->
						<p align="left"><strong>IEEE VR 2024</strong></p>
						<p align="left"><strong>Keywords: Pupil Tracking,Event Cameras.</strong></p>
						<ul class="actions" align="left">
						<li><a href="" class="button icon solid fa-file-pdf"> Coming Soon!</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Test-Time Adaptation for Nighttime Color-Thermal Semantic Segmentation</a></h3>
						</header>
						<a class="image left"><img src="images/NightTTA.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yexin Liu, Weiming Zhang, Guoyang Zhao, Jinjing Zhu, Athanasios Vasilakos, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>IEEE Transactions on Artificial Intelligence</strong></p>
						<p align="left"><strong>Keywords: Night-time segmentation, TTA, Cross-modal learning.</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2307.04470" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Towards Dynamic and Small Objects Refinement for Unsupervised Domain Adaptative Nighttime Semantic Segmentation</a></h3>
						</header>
						<a class="image left"><img src="images/UDA_nightSS.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Jingyi Pan, Sihang Li, Yucheng Chen, Jinjing Zhu, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv </strong></p>
						<p align="left"><strong>Keywords: Nighttime Semantic Segmentation, UDA</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2310.04747.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>


						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">E-CLIP: Towards Label-efficient Event-based Open-world Understanding by CLIP</a></h3>
						</header>
						<a class="image left"><img src="images/E-CLIP.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Jiazhou Zhou, Xu Zheng, Yuanhuiyi Lyu, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv (TPAMI in review)</strong></p>
						<p align="left"><strong>Keywords: CLIP, Open-World Understanding, Event</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2308.03135" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for Panoramic Semantic Segmentation</a></h3>
						</header>
						<a class="image left"><img src="images/Neighbor.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Xu Zheng, Tianbo Pan, Yunhao Luo, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>ICCV 2023</strong></p>
						<p align="left"><strong>Keywords: Panoramic Semantic Segmentation, Unsupervised Domain Adaptation</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Look_at_the_Neighbor_Distortion-aware_Unsupervised_Domain_Adaptation_for_Panoramic_ICCV_2023_paper.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution</a></h3>
						</header>
						<a class="image left"><img src="images/OmniZoomer.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Zidong Cao, Hao Ai, Yan-Pei Cao, Ying Shan, Xiaohu Qie, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>ICCV 2023</strong></p>
						<p align="left"><strong>Keywords: Omnidirectional image, Mobius transformation, Image enhancement</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_OmniZoomer_Learning_to_Move_and_Zoom_in_on_Sphere_at_ICCV_2023_paper.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF</a></h3>
						</header>
						<a class="image left"><img src="images/Exnerf.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Haotian Bai, Yiqi Lin, Yize Chen, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>ICCV 2023</strong></p>
						<p align="left"><strong>Keywords: NeRF, compactness, real-time rendering</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bai_Dynamic_PlenOctree_for_Adaptive_Sampling_Refinement_in_Explicit_NeRF_ICCV_2023_paper.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation</a></h3>
						</header>
						<a class="image left"><img src="images/cnntransformer.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Jinjing Zhu, Yunhao Luo, Xu Zheng, Hao Wang, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>ICCV 2023</strong></p>
						<p align="left"><strong>Keywords: knowledge distillation, mutual learning, Transformer</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_A_Good_Student_is_Cooperative_and_Reliable_CNN-Transformer_Collaborative_Learning_ICCV_2023_paper.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples</a></h3>
						</header>
						<a class="image left"><img src="images/pointcloud.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Qiufan Ji, Lin Wang (corresponding author), Shengshan Hu, Lichao Sun, Cong Shi, Yingying Chen </strong></p>
						<!-- -->
						<p align="left"><strong>ICCV 2023</strong></p>
						<p align="left"><strong>Keywords: Deep Neural Networks, 3D Point Cloud Recognition</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_Benchmarking_and_Analyzing_Robust_Point_Cloud_Recognition_Bag_of_Tricks_ICCV_2023_paper.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>


						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">360∘ High-Resolution Depth Estimation via Uncertainty-aware Structural Knowledge Transfer</a></h3>
						</header>
						<a class="image left"><img src="images/360high.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Zidong Cao, Hao Ai, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv</strong></p>
						<p align="left"><strong>Keywords: omnidirectional image, depth estimation, image super-resolution, knowledge transfer, weakly-supervised learning</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/abs/2304.07967" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback</a></h3>
						</header>
						<a class="image left"><img src="images/3D_Generation.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yiqi Lin, Hao Wu, Ruichen Wang, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv</strong></p>
						<p align="left"><strong>Keywords: Large Language Models, 3D Generative Models, Multi-round Interactions </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2305.15808.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Learning INR for Event-guided Rolling Shutter Frame Correction, Deblur, and Interpolation</a></h3>
						</header>
						<a class="image left"><img src="images/INR.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yunfan Lu, Guoqiang Liang, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv</strong></p>
						<p align="left"><strong>Keywords: Rolling Shutter, Deblur, Frame Interpolation </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2305.15078.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>


						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout</a></h3>
						</header>
						<a class="image left"><img src="images/CompoNeRF.jpg" alt="" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv</strong></p>
						<p align="left"><strong>Keywords: NeRF, text-to-image generation, diffusuion, 3D content </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2303.13843.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>


						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks</a></h3>
						</header>
						<a class="image left"><img src="images/Eventsurvey.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Xu Zheng, Yexin Liu, Yunfan Lu, Tongyan Hua, Tianbo Pan, Weiming Zhang, Dacheng Tao, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv</strong></p>
						<p align="left"><strong>Keywords: event-based vision, deep learning, benchmarks </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2302.08890.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution</a></h3>
						</header>
						<a class="image left"><img src="images/LST.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yunfan Lu, Zipeng Wang, Minjie Liu, Hongjian Wang, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2023</strong></p>
						<p align="left"><strong>Keywords: event-based vision, video super-resolution </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2303.13767.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">HRDFuse: Monocular 360°Depth Estimation by Collaboratively Learning Holistic-with-Regional Depth Distributions</a></h3>
						</header>
						<a class="image left"><img src="images/HRDFuse.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Hao Ai, Zidong cao, Yan-pei Cao, Ying Shan, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2023</strong></p>
						<p align="left"><strong>Keywords: 360 vision, depth estimation </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2303.11616.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective</a></h3>
						</header>
						<a class="image left"><img src="images/Patch.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Jinjing Zhu, Haotian Bai, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2023 (Highlight top 2.5%)</strong></p>
						<p align="left"><strong>Keywords: UDA, game theory </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2303.13434.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>




						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation</a></h3>
						</header>
						<a class="image left"><img src="images/BSDM.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Xu Zheng, Jinjing Zhu, Yexin Liu, Zidong Cao, Chong Fu, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2023</strong></p>
						<p align="left"><strong>Keywords: 360 vision, UDA, Segmentation </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2303.14360.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Vetaverse: A Survey on the Intersection of Metaverse, Vehicles, and Transportation Systems</a></h3>
						</header>
						<a class="image left"><img src="images/Veta.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Pengyuan Zhou, Jinjing Zhu, Yiting Wang, Yunfan Lu, Zixiang Wei, Haolin Shi, Yuchen Ding, Yu Gao, Qinglong Huang, Yan Shi, Ahmad Alhilal, Lik-Hang Lee, Tristan Braud, Pan Hui, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>AAAI 2023</strong></p>
						<p align="left"><strong>Keywords: Metaverse, Transportation </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2210.15109.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>





						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">SEPT: Towards Scalable and Efficient Visual Pre-Training</a></h3>
						</header>
						<a class="image left"><img src="images/SEPT.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yiqi Lin, Huabin Zheng, Huaping Zhong, Jinjing Zhu, Weijia Li, Conghui He, Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>AAAI 2023</strong></p>
						<p align="left"><strong>Keywords: CNN, model pretraining, image classification  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2212.05473.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>




						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Deep Learning for Omnidirectional Vision: A Survey and New Perspectives</a></h3>
						</header>
						<a class="image left"><img src="images/Omni.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Hao Ai*, Zidong Cao˚, Jinjing Zhu, Haotian Bai, Yucheng Chen, and Lin Wang</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv (TPAMI in review)</strong></p>
						<p align="left"><strong>Keywords: 360 vision, deep learning, survey and analysis  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2205.10468.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>


						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">All One Needs to Know about Priors for Deep Image Restoration and Enhancement: A Survey</a></h3>
						</header>
						<a class="image left"><img src="images/pami-prior.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Changgyoon Oh, Wonjune Cho, Daehee Park, Yujeong Chae, Lin Wang, Kuk-Jin Yoon</strong></p>
						<!-- -->
						<p align="left"><strong>Arxiv (TPAMI in review)</strong></p>
						<p align="left"><strong>Keywords: Prior, Image restoration and  enhancement, survey and alalysis </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2206.02070.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Efficient Video Deblurring Guided by Motion Magnitude</a></h3>
						</header>
						<a class="image left"><img src="images/eccv_mmp.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Yusheng Wang, Yunfan Lu, Ye Gao, Lin Wang, Zhihang Zhong, Yinqiang Zheng, and Atsushi Yamashita</strong></p>
						<!-- -->
						<p align="left"><strong>ECCV 2022</strong></p>
						<p align="left"><strong>Keywords: Video deblurring, motion magnitude</strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2112.06179.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">BIPS: Bi-modal Indoor Panorama Synthesis via Residual Depth-aided Adversarial Learning</a></h3>
						</header>
						<a class="image left"><img src="images/bips.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Changgyoon Oh, Wonjune Cho, Daehee Park, Yujeong Chae, Lin Wang, Kuk-Jin Yoon</strong></p>
						<!-- -->
						<p align="left"><strong>ECCV 2022</strong></p>
						<p align="left"><strong>Keywords: Panorama synthesis,  bi-modal fusion, adversarial learning  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2112.06179.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>




						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Event-guided Deblurring of Unknown Exposure Time Videos</a></h3>
						</header>
						<a class="image left"><img src="images/deblur_event.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Taewoo Kim, Jungmin Lee, Lin Wang and Kuk-Jin Yoon</strong></p>
						<!-- -->
						<p align="left"><strong>ECCV 2022</strong></p>
						<p align="left"><strong>Keywords: Event-based vision, video deblurring, video frame acquisition  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2112.06988.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>



						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">SphereSR: 360° Image Super-Resolution with Arbitrary Projection via Continuous Spherical Image Representation</a></h3>
						</header>
						<a class="image left"><img src="images/sphereSR.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Youngho Yoon, Lin Wang, Inchul Chung and Kuk-Jin Yoon</strong></p>
						<!-- -->
						<p align="left"><strong>CVPR 2022</strong></p>
						<p align="left"><strong>Keywords: Ominidirectional vision,  Image super-resolution, Continuous image representation  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2112.06179.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>




						<!-- Highlight -->
						<article class="box highlight">
						<header>
						<h3><a href="#">Deep Learning for HDR Imaging: State-of-the-Art and Future Trends</a></h3>
						</header>
						<a class="image left"><img src="images/hdr_img.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
						<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
						<!-- -->
						<p align="left"><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI, IF: 17.861)</strong></p>
						<p align="left"><strong>Keywords: High dynamic range imaging, Convolutional Neural Networks (CNNs), Deep Learning  </strong></p>
						<ul class="actions" align="left">
						<li><a href="https://arxiv.org/pdf/2110.10394.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
						</ul>
						</article>

						<!-- Highlight -->
							<article class="box highlight">
								<header>
									<h3><a href="#">All One Needs to Know about Metaverse: A Complete Survey on Technological Singularity, Virtual Ecosystem, and Research Agenda</a></h3>
								</header>
								<a class="image left"><img src="images/metaverse.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
						    <div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
								<p align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></p>
							<!-- -->
								<p align="left"><strong>The proceedings of IEEE (IF: 10.961)</strong></p>
								<p align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></p>
								<ul class="actions" align="left">
								<li><a href="https://tuhat.helsinki.fi/ws/portalfiles/portal/169348619/METAVERSE.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
								</ul>
								</article>

								<!-- Highlight -->
								<article class="box highlight">
								<header>
								<h3><a href="#">SiamEvent:Event-based Object Tracking via Edge-aware Similarity Learning with Siamese Networks</a></h3>
								</header>
								<a class="image left"><img src="images/siamevent.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
									<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
									<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
									<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
								<p align="left"><strong>Yujeon Chae, Lin Wang and  Kuk-Jin Yoon</strong></p>
								<!-- -->
								<p align="left"><strong>IEEE RA-L and ICRA, 2022.</strong></p>
								<p align="left"><strong>Keywords: Event-based vision, object tracking</strong></p>
								<ul class="actions" align="left">
								<li><a href="https://arxiv.org/pdf/2109.13456.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
								</ul>
								</article>

							<!-- Highlight -->
							<article class="box highlight">
								<header>
									<h3><a href="#">Semi-supervised Transfer Learning for Single Image Super-Resolution via Feature Augmentation and Inverse Consistency Regularization</a></h3>
								</header>
								<a class="image left"><img src="images/dbep_results.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
						    <div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
								<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
							<!-- -->
								<p align="left"><strong>the Association for the Advancement of Artificial Intelligence (AAAI), 2022.</strong></p>
								<p align="left"><strong>Keywords: Image super-resolution, Semi-supervised learning, transfer learning</strong></p>
								<ul class="actions" align="left">
								<li><a href="#" class="button icon solid fa-file-pdf"> Check our paper</a></li>
								</ul>
								</article>


								<!-- Highlight -->
								<article class="box highlight">
								<header>
								<h3><a href="#">Joint Framework for Intensity Image Reconstruction and Super-Resolution with an Event Camera</a></h3>
								</header>
								<a class="image left"><img src="images/eventsr_PAMI.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
									<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
									<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
									<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
								<p align="left"><strong>Lin Wang, Tae-Kyun Kim and Kuk-Jin Yoon</strong></p>
								<!-- -->
								<p align="left"><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI, IF: 17.861)</strong></p>
								<p align="left"><strong>Keywords: Adversarial attack, Holistic scene understanding, Multi-task learning, Generative model </strong></p>
								<ul class="actions" align="left">
								<li><a href="https://ieeexplore.ieee.org/abstract/document/9524508" class="button icon solid fa-file-pdf"> Check our paper</a></li>
								</ul>
								</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">PSAT-GAN: Efficient Adversarial Attacks against Holistic Scene Understanding</a></h3>
							</header>
							<a class="image left"><img src="images/psatgan.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong> IEEE Transactions on Image Processing (TIP,  IF 10.856)</strong></p>
							<p align="left"><strong>Keywords: Adversarial attack, Holistic scene understanding, Multi-task learning, Generative model </strong></p>
							<ul class="actions" align="left">
							<li><a href="https://ieeexplore.ieee.org/document/9524508" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Semi-supervised Student-Teacher Learning for Single Image Super-Resolution</a></h3>
							</header>
							<a class="image left"><img src="images/semi_sup_PR.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>Pattern Recognition (PR, IF 7.74)</strong></p>
							<p align="left"><strong>Keywords: Image super-resolution, Semi-supervised learning, Student-Teacher learning, Adversarial Learning.</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320321003873" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Dual Transfer Learning for Event-based End-task Prediction via Pluggable Image Translation</a></h3>
							</header>
							<a class="image left"><img src="images/dtl.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang, YuJeon Chae and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>International Conference on Computer Vision (ICCV), 2021.</strong></p>
							<p align="left"><strong>Keywords: Event-based vision, dual-transfer learning, event to image translation</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Dual_Transfer_Learning_for_Event-Based_End-Task_Prediction_via_Pluggable_Event_ICCV_2021_paper.pdf" class="button icon solid fa-file"> Check our paper</a></li>
							</ul>
							</article>

							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">GraphShop: Graph-based Approach for Shop-type Recommendation</a></h3>
							</header>
							<a class="image left"><img src="images/graphshop.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Guoyuan An, Sungeui Yoon,  Jaeyoon Kim, Lin Wang and Myoungho Kim </strong></p>
							<!-- -->
							<p align="left"><strong>SIAM International Conference on Data Mining</strong></p>
							<p align="left"><strong>Keywords: shop-type recommendation, graph neural network, smart city, recommender system </strong></p>
							<ul class="actions" align="left">
							<li><a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611976700.7" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">EvDistill: Asynchronous Events to End-task Learning via Bidirectional Reconstruction-guided Cross-modal Knowledge Distillation</a></h3>
							</header>
							<a class="image left"><img src="images/evdistill.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang, YuJeon Chae, Sunghoon Yoon, Tae-Kyun Kim and  Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>The Conference on Computer Vision and Pattern Recognition (CVPR)  2021</strong></p>
							<p align="left"><strong>Keywords: Event-based vision, Knowledge distillation, Semantic segmentation, Generative model</strong></p>
							<ul class="actions" align="left">
							<li><a href="http://vi.kaist.ac.kr/wp-content/uploads/2021/03/EventSeg_S_Tlearning.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks </a></h3>
							</header>
							<a class="image left"><img src="images/kd_pami.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI, IF 17.861)</strong></p>
							<p align="left"><strong>Keywords: knowledge distillation, student-teacher learning, visual intelligence</strong></p>
							<ul class="actions" align="left">
							<li><a href="http://vi.kaist.ac.kr/wp-content/uploads/2021/03/EventSeg_S_Tlearning.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Learning to Reconstruct HDR Images from Events, with Applications to Depth and Flow</a></h3>
							</header>
							<a class="image left"><img src="images/eventgan_ijcv.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>S. Mohammad Mostafavi I.,  Lin Wang, and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>International Journal of Computer  Vision (IJCV, IF: 7.410)</strong></p>
							<p align="left"><strong>Keywords: Event-based vision, HDR image reconstruction, depth and flow estimation</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://link.springer.com/article/10.1007/s11263-020-01410-2" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">EventSR: from asynchronous events to image reconstruction, restoration, and super-resolution via end-to-end adversarial learning</a></h3>
							</header>
							<a class="image left"><img src="images/eventsr.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
								<!-- <div class="a" align="left"><strong>Lik-Hang Lee, Tristan Braud*, Pengyuan Zhou*, Lin Wang*, Dianlei Xu*, Zijun Lin*, Abhishek Kumar*, Carlos Bermejo*, Pan Hui (*Co-authorship)</strong></div>
								<div class="a" align="left"><strong>Submitted to the proceedings of IEEE</strong></div>
								<div class="a" align="left"><strong>Keywords: Metaverse, interaction techniques, AI</strong></div> -->
							<p align="left"><strong>Lin Wang, Tae-Kyun Kim and Kuk-Jin Yoon</strong></p>
							<!-- -->
							<p align="left"><strong>The Conference on Computer Vision and Pattern Recognition (CVPR) 2020</strong></p>
							<p align="left"><strong>Keywords: Event-based vision, image reconstruction, restoration, super-resolution, GAN</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_EventSR_From_Asynchronous_Events_to_Image_Reconstruction_Restoration_and_Super-Resolution_CVPR_2020_paper.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Deceiving Image-to-Image Translation Networks for Autonomous Driving With Adversarial Perturbations</a></h3>
							</header>
							<a class="image left"><img src="images/adv_icra.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
							<p align="left"><strong>Lin Wang, Wonjune Cho and Kuk-Jin Yoon</strong></p>
							<p align="left"><strong>International Conference on Robotics and Automation (ICRA 2020) and IEEE Robotics and Automation Letters (RA-L,  IF: 3.608)</strong></p>
							<p align="left"><strong>Keyworks: adversarial attack,  Image to image translation, Autonomous driving</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://ieeexplore.ieee.org/document/8962221" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>

							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Event-based High Dynamic Range Image and Very High Frame Rate Video Generation using Conditional Generative Adversarial Networks</a></h3>
							</header>
							<a class="image left"><img src="images/eventgan_cvpr.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
							<p align="left"><strong>Lin Wang, S. Mohammad Mostafavi I., Yo-Sung Ho, and Kuk-Jin Yoon</strong></p>
							<p align="left"><strong>The Conference on Computer Vision and Pattern Recognition (CVPR)  2019</strong></p>
							<p align="left"><strong>Keywords: event-based vision, image reconstruction, high-framerate video, GAN</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Event-Based_High_Dynamic_Range_Image_and_Very_High_Frame_Rate_CVPR_2019_paper.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>

							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Co-DesignMR: A MR-based Interactive Workstation Design System Supporting Collaboration</a></h3>
							</header>
							<a class="image left"><img src="images/coaugMR.png" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
							<p align="left"><strong>Lin Wang and Kuk-Jin Yoon</strong></p>
							<p align="left"><strong>Arxiv 2020</strong></p>
							<p align="left"><strong>Keywords: Mixed reality, ergonomics, product design</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://arxiv.org/pdf/1907.03107.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>

							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">Visual simulation of a capsizing ship in stormy weather condition</a></h3>
							</header>
							<a class="image left"><img src="images/vis_comp.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
							<p align="left"><strong>Lin Wang and Soonhung Han</strong></p>
							<p align="left"><strong>The Visual Computer </strong></p>
							<p align="left"><strong>Keywords: Ship capsize, fluid simulation, visual effects</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://link.springer.com/content/pdf/10.1007/s00371-018-1579-6.pdf" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>


							<!-- Highlight -->
							<article class="box highlight">
							<header>
							<h3><a href="#">A visual simulation of ocean floating wind power system</a></h3>
							</header>
							<a class="image left"><img src="images/float.jpg" alt="", align="left" style="width:400px;height:250px;margin-right:15px;"></a>
							<p align="left"><strong>Lin Wang, Hyuncheol Kim, Imgyu Kim and Soonhung Han</strong></p>
							<p align="left"><strong>Computer Animation and Virtual Worlds</strong></p>
							<p align="left"><strong>Keywords: Wind turbine design, visual simulation, physically-based fluid simulation</strong></p>
							<ul class="actions" align="left">
							<li><a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/cav.1859" class="button icon solid fa-file-pdf"> Check our paper</a></li>
							</ul>
							</article>







					</li>


				</ul>
			</section>





		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
